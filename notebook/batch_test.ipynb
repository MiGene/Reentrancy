{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.preprocessing import MinMaxScaler, MaxAbsScaler, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from utils import var_setup\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DATA_PATH = \"./dataset/100tx/smoted/\"\n",
    "TEST_DATA_PATH = \"./dataset/100tx/fold/\"\n",
    "PERFORMANCE_PATH = \"./data/100tx/performance.csv\"\n",
    "FEATURE_IMPORTANCE_PATH = \"./data/100tx/feature_importance.csv\"\n",
    "PREDICTION_PATH = \"./data/100tx/prediction/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_list_preserve_sequence(list, to_remove):\n",
    "    new_list = [item for item in list if item not in to_remove]\n",
    "    return new_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"./dataset/real-time/fold/1.csv\"\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "columns = df.columns.tolist()\n",
    "\n",
    "feature_with_sus = remove_list_preserve_sequence(\n",
    "    df.columns,\n",
    "    [\n",
    "        \"transaction_hash\",\n",
    "        \"from_address\",\n",
    "        \"to_address\",\n",
    "        \"block_timestamp\",\n",
    "        \"add_feat_hash\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "feature = remove_list_preserve_sequence(\n",
    "    df.columns,\n",
    "    [\n",
    "        \"transaction_hash\",\n",
    "        \"from_address\",\n",
    "        \"to_address\",\n",
    "        \"block_timestamp\",\n",
    "        \"add_feat_hash\",\n",
    "        \"is_sus\",\n",
    "    ],\n",
    ")\n",
    "# print(len(features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def var():\n",
    "    variables = {}\n",
    "\n",
    "    #### feature ###\n",
    "\n",
    "    variables[\"feature_with_sus\"] = feature_with_sus\n",
    "    variables[\"feature\"] = feature\n",
    "    # remove_feature = [\n",
    "    #     \"contract_lifetime_block\",\n",
    "    #     \"contract_lifetime_days\",\n",
    "    #     \"sender_tx_count_call_contract\",\n",
    "    #     \"sender_lifetime_days\",\n",
    "    #     \"sender_lifetime_block\",\n",
    "    #     \"contract_main_active_days\",\n",
    "    #     \"sender_main_active_days\",\n",
    "    #     \"value\",\n",
    "    #     \"contract_block_ratio\",\n",
    "    #     \"tx_sender_call_contract\",\n",
    "    # ]\n",
    "    # remove_feature = [  # for n=11\n",
    "    #     \"contract_lifetime_block\",\n",
    "    #     \"contract_lifetime_days\",\n",
    "    #     \"sender_tx_count_call_contract\",\n",
    "    #     \"sender_lifetime_days\",\n",
    "    #     \"sender_lifetime_block\",\n",
    "    #     \"contract_main_active_days\",\n",
    "    #     \"sender_main_active_days\",\n",
    "    #     \"value\",\n",
    "    #     \"contract_block_ratio\",\n",
    "    #     \"tx_sender_call_contract\",\n",
    "    # ]\n",
    "    remove_feature = [\n",
    "        # \"contract_main_active_days\",\n",
    "        \"sender_main_active_days\",\n",
    "        \"contract_lifetime_block\",\n",
    "        \"contract_lifetime_days\",\n",
    "        \"sender_lifetime_days\",\n",
    "        \"sender_lifetime_block\",\n",
    "    ]\n",
    "    # remove_feature = [  # for 9_new\n",
    "    #     \"contract_lifetime_block\",\n",
    "    #     \"contract_lifetime_days\",\n",
    "    #     \"sender_tx_count_call_contract\",\n",
    "    #     \"sender_lifetime_days\",\n",
    "    #     \"sender_lifetime_block\",\n",
    "    #     \"contract_main_active_days\",\n",
    "    #     \"sender_main_active_days\",\n",
    "    #     \"contract_block_ratio\",\n",
    "    #     \"tx_sender_call_contract\",\n",
    "    #     \"contract_block_involved\",\n",
    "    #     \"value\",\n",
    "    # ]\n",
    "\n",
    "    # remove_feature = [\n",
    "    #     \"contract_lifetime_block\",\n",
    "    #     \"contract_lifetime_days\",\n",
    "    #     \"sender_tx_count_call_contract\",\n",
    "    #     \"sender_lifetime_days\",\n",
    "    #     \"sender_lifetime_block\",\n",
    "    #     \"contract_main_active_days\",\n",
    "    #     # \"value\",\n",
    "    #     \"contract_block_ratio\",\n",
    "    #     \"tx_sender_call_contract\",\n",
    "    #     # \"tx_count_call_contract_per_days\",\n",
    "    #     \"sender_active_days_ratio\",\n",
    "    #     \"sender_block_ratio\",\n",
    "    #     \"distinct_sender_called_in_sample\",\n",
    "    #     \"sender_tx_count_call_contract\",\n",
    "    #     # \"contract_block_involved\",\n",
    "    #     # \"gas\",\n",
    "    #     # \"contract_tx_count\",\n",
    "    #     # \"sender_block_involved\",\n",
    "    #     # \"sender_tx_count\",\n",
    "    #     # \"contract_interact\",\n",
    "    #     # \"sender_days_call_contract\",\n",
    "    #     # \"distinct_sender_in_contract\",\n",
    "    #     # \"depth\",\n",
    "    #     # \"gas_price\",\n",
    "    #     # \"receipt_cumulative_gas_used\",\n",
    "    #     # \"nonce\",\n",
    "    #     # \"contract_active_day_ratio\",\n",
    "    #     # \"tx_count_per_distinct_caller\",\n",
    "    # ]\n",
    "    variables[\"target_feature\"] = remove_list_preserve_sequence(feature, remove_feature)\n",
    "    variables[\"target_feature_with_sus\"] = remove_list_preserve_sequence(\n",
    "        feature_with_sus, remove_feature\n",
    "    )\n",
    "    variables[\"z_score_feature\"] = [\n",
    "        member for member in variables[\"target_feature\"] if member.startswith(\"z_\")\n",
    "    ]\n",
    "\n",
    "    variables[\"static_feature\"] = remove_list_preserve_sequence(\n",
    "        variables[\"target_feature\"], variables[\"z_score_feature\"]\n",
    "    )\n",
    "\n",
    "    variables[\"static_feature_with_sus\"] = remove_list_preserve_sequence(\n",
    "        variables[\"feature_with_sus\"], variables[\"z_score_feature\"]\n",
    "    )\n",
    "\n",
    "    # best feature\n",
    "    variables[\"best_feature\"] = []\n",
    "\n",
    "    return variables\n",
    "\n",
    "def random_forest():\n",
    "    # return \"\"\n",
    "    static_feature = var_setup.var()[\"static_feature\"]\n",
    "    z_features = var_setup.var()[\"z_score_feature\"]\n",
    "    features = static_feature + z_features\n",
    "\n",
    "    fold_files = [\"1.csv\", \"2.csv\", \"3.csv\", \"4.csv\", \"5.csv\"]\n",
    "    matching_percentage = []\n",
    "    f1_percentage = []\n",
    "\n",
    "    num_pipeline = Pipeline(\n",
    "        steps=[\n",
    "            (\"scale\", MinMaxScaler()),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    col_trans = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"num_pipeline\", num_pipeline, static_feature),\n",
    "            (\"passthrough\", \"passthrough\", z_features),\n",
    "        ],\n",
    "        n_jobs=1,\n",
    "    )\n",
    "\n",
    "    feature_importance_df = pd.DataFrame(columns=features)\n",
    "\n",
    "    performance_df = pd.DataFrame(\n",
    "        columns=[\n",
    "            \"Fold\",\n",
    "            \"Total Values\",\n",
    "            \"Matching\",\n",
    "            \"Matching Percentage\",\n",
    "            \"Actual Attack\",\n",
    "            \"Predicted\",\n",
    "            \"True Positive\",\n",
    "            \"False Positive\",\n",
    "            \"False Negative\",\n",
    "            \"F1 Score\",\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    temp = pd.read_csv(TRAIN_DATA_PATH + \"1.csv\")\n",
    "\n",
    "    for i in range(len(fold_files)):\n",
    "        file_path_train = TRAIN_DATA_PATH\n",
    "        file_path_test = TEST_DATA_PATH\n",
    "\n",
    "        train_data = pd.DataFrame(columns=temp.columns)\n",
    "\n",
    "        for j in range(len(fold_files)):\n",
    "            if j == i:\n",
    "                continue\n",
    "            df = pd.read_csv(file_path_train + fold_files[j])\n",
    "            train_data = pd.concat([train_data, df], ignore_index=True)\n",
    "\n",
    "        train_data = train_data.sort_index()\n",
    "        X = train_data[features]\n",
    "        X = X.sort_index()\n",
    "        y = train_data.is_sus.astype(int)\n",
    "        y = y.sort_index()\n",
    "\n",
    "        rf_model = RandomForestClassifier(\n",
    "            n_estimators=5,  # 11\n",
    "            max_depth=5,  # 5\n",
    "            min_samples_leaf=50,\n",
    "            random_state=42,\n",
    "            n_jobs=1,\n",
    "        )  # 50\n",
    "        xgb_model = xgb.XGBClassifier(\n",
    "            n_estimators=5,\n",
    "            max_depth=5,  # 5\n",
    "            min_samples_leaf=50,\n",
    "            random_state=42,\n",
    "            n_jobs=1,\n",
    "        )\n",
    "        lgbm_model = lgb.LGBMClassifier(\n",
    "            n_estimators=5,\n",
    "            max_depth=5,  # 5\n",
    "            min_samples_leaf=50,\n",
    "            random_state=42,\n",
    "            n_jobs=1,\n",
    "        )\n",
    "        model_pipeline = Pipeline(\n",
    "            steps=[(\"col_trans\", col_trans), (\"model\", xgb_model)]\n",
    "        )\n",
    "        model_pipeline.fit(X, y)\n",
    "\n",
    "        final_estimator = model_pipeline.steps[-1][1]\n",
    "        if isinstance(final_estimator, RandomForestClassifier):\n",
    "            feature_importances = model_pipeline.named_steps[\n",
    "                \"model\"\n",
    "            ].feature_importances_\n",
    "            feature_importance_df = feature_importance_df._append(\n",
    "                pd.DataFrame([feature_importances], columns=features),\n",
    "                ignore_index=True,\n",
    "            )\n",
    "\n",
    "        return model_pipeline\n",
    "    \n",
    "\n",
    "def test(df,model):\n",
    "    test_data = pd.read_csv(file_path_test + fold_files[i])\n",
    "    # test_data[\"contract_main_active_days\"] = 1\n",
    "    test_data = test_data.sort_index()\n",
    "    test_X = test_data[features]\n",
    "    test_X = test_X.sort_index()\n",
    "    test_y = test_data.is_sus.astype(int)\n",
    "    test_y = test_y.sort_index()\n",
    "    predict_y = model_pipeline.predict(test_X)\n",
    "\n",
    "    fold_data_df = pd.concat(\n",
    "        [\n",
    "            test_data,\n",
    "            pd.DataFrame({\"True Labels\": test_y, \"Predicted Labels\": predict_y}),\n",
    "        ],\n",
    "        axis=1,\n",
    "    )\n",
    "    fold_data_df.to_csv(PREDICTION_PATH + f\"{i+1}.csv\", index=False)\n",
    "\n",
    "    matching_values = np.sum(np.array(test_y) == np.array(predict_y))\n",
    "    total_samples = len(test_y)\n",
    "    percentage_matching = (matching_values / total_samples) * 100\n",
    "    matching_percentage.append(percentage_matching)\n",
    "    actualAttack = sum(test_y[test_y == 1])\n",
    "    predicted = sum(predict_y[predict_y == 1])\n",
    "    TP = sum((test_y == 1) & (predict_y == 1))\n",
    "    FP = sum((test_y == 0) & (predict_y == 1))\n",
    "    FN = sum((test_y == 1) & (predict_y == 0))\n",
    "\n",
    "    print(f\"************** Fold {i+1} **************\")\n",
    "    print(\"--- Overall Performance ---\")\n",
    "    print(\"Total Values :\", total_samples)\n",
    "    print(\"Matching :\", matching_values)\n",
    "    print(\"Matching percentage :\", percentage_matching)\n",
    "    print(\"--- Caught Performance ---\")\n",
    "    print(\"Actual Attack :\", actualAttack)\n",
    "    print(\"Predicted :\", predicted)\n",
    "    print(\"True Positive(attack caught) :\", TP)\n",
    "    print(\"False Positive :\", FP)\n",
    "    print(\"False Negative :\", FN)\n",
    "    f1Score = f1_score(test_y, predict_y)\n",
    "\n",
    "    f1_percentage.append(f1Score)\n",
    "    print(\"F1 score : \", f1Score)\n",
    "    print(\"\\n\")\n",
    "\n",
    "    performance_df = performance_df._append(\n",
    "        {\n",
    "            \"Fold\": i,\n",
    "            \"Total Values\": total_samples,\n",
    "            \"Matching\": matching_values,\n",
    "            \"Matching Percentage\": percentage_matching,\n",
    "            \"Actual Attack\": actualAttack,\n",
    "            \"Predicted\": predicted,\n",
    "            \"True Positive\": TP,\n",
    "            \"False Positive\": FP,\n",
    "            \"False Negative\": FN,\n",
    "            \"F1 Score\": f1Score,\n",
    "        },\n",
    "        ignore_index=True,\n",
    "    )\n",
    "\n",
    "performance_df.to_csv(PERFORMANCE_PATH, index=False)\n",
    "average_result = np.mean(matching_percentage)\n",
    "average_result2 = np.mean(f1_percentage)\n",
    "\n",
    "print(f\"Average Result Across Folds: {average_result}\")\n",
    "print(f\"Average Result Across Folds: {average_result2}\")\n",
    "feature_importance_df.to_csv(FEATURE_IMPORTANCE_PATH, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest()\n",
    "# # *\n",
    "# #     # base_estimator = DecisionTreeClassifier(max_depth=5, min_samples_leaf=50, random_state=42)\n",
    "# #     # adaboost_model = AdaBoostClassifier(estimator=base_estimator,\n",
    "# #     #                                 n_estimators=250, # Adjust as needed\n",
    "# #     #                                 random_state=42)\n",
    "# #     xgb_model = XGBClassifier(\n",
    "# #         # objective=\"binary:logistic\",  # Adjust for binary or multiclass classification\n",
    "# #         # n_estimators=250,  # Number of boosting rounds\n",
    "# #         # max_depth=7,  # Maximum depth of trees\n",
    "# #         # learning_rate=0.1,  # Step size shrinkage\n",
    "# #         # scale_pos_weight=len(y[y == 0]) / len(y[y == 1]),  # Handling imbalance\n",
    "# #         # use_label_encoder=False,  # Avoid warnings with recent xgboost versions\n",
    "# #         # random_state=42,\n",
    "# #     )\n",
    "# #     gbm_model = LGBMClassifier(\n",
    "# #         # num_leaves=10,\n",
    "# #         # n_estimators=250,  # Number of boosting rounds\n",
    "# #         # max_depth=5,  # Maximum depth of each tree\n",
    "# #         # learning_rate=0.1,  # Learning rate\n",
    "# #         # class_weight=\"balanced\",  # Handle class imbalance\n",
    "# #         # random_state=42,\n",
    "# #     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
