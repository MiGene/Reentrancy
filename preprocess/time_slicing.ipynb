{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from utils import merge_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_interactions(transaction_info):\n",
    "    # Ensure block_timestamp is in datetime format for sorting\n",
    "    transaction_info['block_timestamp'] = pd.to_datetime(transaction_info['block_timestamp'])\n",
    "    \n",
    "    # Sort by 'to_address' and 'block_timestamp'\n",
    "    transaction_info = transaction_info.sort_values(by=['to_address', 'block_timestamp'])\n",
    "\n",
    "    # Create a new column 'contract_interact' which mimics ROW_NUMBER() OVER (PARTITION BY to_address ORDER BY block_timestamp)\n",
    "    transaction_info['contract_interact'] = transaction_info.groupby('to_address').cumcount() + 1\n",
    "\n",
    "    # Select relevant columns and rename 'transaction_hash' to 'interact_hash'\n",
    "    # interactions = transaction_info[['transaction_hash']].rename(columns={'transaction_hash': 'interact_hash'})\n",
    "    # interactions['contract_interact'] = transaction_info['contract_interact']\n",
    "\n",
    "    return transaction_info\n",
    "\n",
    "def time_slice_df(df, num_rows=50, time_col='block_timestamp', sus_col='is_sus', how='last'):\n",
    "    # Convert 'time_col' to datetime format\n",
    "    df[time_col] = pd.to_datetime(df[time_col])\n",
    "\n",
    "    # Sort by 'time_col' in descending order (from last to first)\n",
    "    df = df.sort_values(by=time_col, ascending=False).reset_index()\n",
    "\n",
    "    # Find the index of the first row where 'is_sus' == 1\n",
    "    sus_index = df[df[sus_col] == 1].index[0] if not df[df[sus_col] == 1].empty else 0\n",
    "    # print(df.iloc[sus_index])\n",
    "\n",
    "    # Slice the DataFrame starting from the 'sus_index'\n",
    "    df_sliced = df.iloc[sus_index:sus_index + num_rows]\n",
    "\n",
    "    return df_sliced\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ../dataset/real-time/train_preprocessed\\realtime_bacon.csv\n",
      "Processing ../dataset/real-time/train_preprocessed\\realtime_cream.csv\n",
      "Processing ../dataset/real-time/train_preprocessed\\realtime_dfx.csv\n",
      "Processing ../dataset/real-time/train_preprocessed\\realtime_fei.csv\n",
      "Processing ../dataset/real-time/train_preprocessed\\realtime_hypebear.csv\n",
      "Processing ../dataset/real-time/train_preprocessed\\realtime_jay.csv\n",
      "Processing ../dataset/real-time/train_preprocessed\\realtime_noodle.csv\n",
      "Processing ../dataset/real-time/train_preprocessed\\realtime_omni.csv\n",
      "Processing ../dataset/real-time/train_preprocessed\\realtime_orion.csv\n",
      "Processing ../dataset/real-time/train_preprocessed\\realtime_rari.csv\n",
      "Processing ../dataset/real-time/train_preprocessed\\realtime_revest.csv\n",
      "Processing ../dataset/real-time/train_preprocessed\\realtime_sanshu.csv\n",
      "Processing ../dataset/real-time/train_preprocessed\\realtime_visor.csv\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "\n",
    "folder_path = '../dataset/real-time/train_preprocessed'  # Change this to your folder path\n",
    "dataset_name = folder_path.split('/')[2]\n",
    "# Use glob to get all CSV files in the folder\n",
    "csv_files = glob.glob(f'{folder_path}/*.csv')\n",
    "suffix = '100'\n",
    "# Loop through the CSV files\n",
    "for file in csv_files:\n",
    "    # Read each CSV file into a DataFrame\n",
    "    df = pd.read_csv(file)\n",
    "    print(f'Processing {file}')\n",
    "    df_top_50 = time_slice_df(df,num_rows=100)\n",
    "    df_top_50 = create_interactions(df_top_50)\n",
    "\n",
    "    new_dir = folder_path.replace(dataset_name,f'{dataset_name}-{suffix}')\n",
    "    new_path = file.replace(dataset_name,f'{dataset_name}-{suffix}')\n",
    "    os.makedirs(new_dir, exist_ok=True)\n",
    "    df_top_50.to_csv(new_path,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# import glob\n",
    "# import os\n",
    "\n",
    "# folder_path = '../50_dataset/real-time/train_preprocessed'  # Change this to your folder path\n",
    "\n",
    "# # Use glob to get all CSV files in the folder\n",
    "# csv_files = glob.glob(f'{folder_path}/*.csv')\n",
    "# prefix = '50_'\n",
    "# # Loop through the CSV files\n",
    "# for file in csv_files:\n",
    "#     # Read each CSV file into a DataFrame\n",
    "#     df = pd.read_csv(file)\n",
    "#     # print(f'Processing {file}')\n",
    "#     # df_top_50 = time_slice_df(df)\n",
    "\n",
    "#     # new_dir = folder_path.replace('../','../'+prefix)\n",
    "#     # new_path = file.replace('../','../'+prefix)\n",
    "#     # os.makedirs(new_dir, exist_ok=True)\n",
    "#     new_contract_interact = create_interactions(df)\n",
    "#     new_contract_interact.to_csv(file,index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
